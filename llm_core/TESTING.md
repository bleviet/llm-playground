# Testing Guide for LLM Playground

This document explains the testing strategy and best practices used in this project.

## ğŸ¯ Philosophy

> "Tests are not just about catching bugsâ€”they're about documenting behavior and enabling confident changes."

Our testing approach:
- **Fast by default** - Unit tests with mocks run in milliseconds
- **Reliable** - Tests don't depend on external services
- **Educational** - Tests demonstrate how to use the code
- **Practical** - Tests catch real issues

## ğŸ“ Test Structure

```
llm_core/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # Shared fixtures
â”œâ”€â”€ test_providers.py              # Provider unit tests
â””â”€â”€ test_strategies.py             # Strategy unit tests

applications/summarize_webpage/tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                    # App-specific fixtures
â”œâ”€â”€ test_scraper.py                # Scraping tests
â””â”€â”€ test_summarizer.py             # Workflow tests
```

## ğŸš€ Running Tests

### Install Test Dependencies

```bash
# Install llm_core with dev dependencies
cd llm_core
uv pip install -e ".[dev]"

# Or with pip
pip install -e ".[dev]"
```

### Run All Tests

```bash
# From project root
pytest

# With coverage report
pytest --cov=llm_core --cov-report=html

# With verbose output
pytest -v
```

### Run Specific Test Categories

```bash
# Only unit tests (fast, no external dependencies)
pytest -m unit

# Only integration tests (slow, requires API keys)
pytest -m integration

# Skip integration tests (recommended for CI)
pytest -m "not integration"

# Run tests for a specific module
pytest llm_core/tests/test_providers.py

# Run a specific test
pytest llm_core/tests/test_providers.py::TestProviderInitialization::test_ollama_default_initialization
```

## ğŸ“Š Test Categories

### Unit Tests (`@pytest.mark.unit`)

**Purpose**: Test individual components in isolation

**Characteristics**:
- âœ… Fast (< 1 second total)
- âœ… No network calls
- âœ… Use mocks for external dependencies
- âœ… Deterministic (same input â†’ same output)

**Example**:
```python
@pytest.mark.unit
def test_provider_initialization():
    provider = OpenAIProvider(model_name="gpt-4o-mini")
    assert provider.model == "gpt-4o-mini"
```

### Integration Tests (`@pytest.mark.integration`)

**Purpose**: Test interactions with real external services

**Characteristics**:
- âš ï¸ Slow (API latency)
- âš ï¸ Requires API keys
- âš ï¸ May cost money
- âš ï¸ Can be flaky (network issues)

**Example**:
```python
@pytest.mark.integration
@pytest.mark.slow
def test_openai_real_api(skip_if_no_api_keys):
    skip_if_no_api_keys("openai")
    provider = OpenAIProvider()
    # ... make real API call ...
```

## ğŸ“ Best Practices Demonstrated

### 1. Use Fixtures for Reusable Setup

**Bad** âŒ:
```python
def test_one():
    mock_client = Mock()
    mock_client.chat.completions.create.return_value = Mock()
    # ... test logic ...

def test_two():
    mock_client = Mock()  # Duplicate setup!
    mock_client.chat.completions.create.return_value = Mock()
    # ... test logic ...
```

**Good** âœ…:
```python
@pytest.fixture
def mock_openai_client():
    mock_client = Mock()
    mock_client.chat.completions.create.return_value = Mock()
    return mock_client

def test_one(mock_openai_client):
    # Use fixture
    
def test_two(mock_openai_client):
    # Reuse same setup
```

### 2. Parametrize Tests for Multiple Cases

**Bad** âŒ:
```python
def test_ollama_custom_model():
    provider = OllamaProvider(model_name="model1")
    assert provider.model == "model1"

def test_ollama_another_model():
    provider = OllamaProvider(model_name="model2")
    assert provider.model == "model2"
```

**Good** âœ…:
```python
@pytest.mark.parametrize("model_name", [
    "model1",
    "model2",
    "model3",
])
def test_ollama_custom_models(model_name):
    provider = OllamaProvider(model_name=model_name)
    assert provider.model == model_name
```

### 3. Test the Interface, Not the Implementation

**Bad** âŒ:
```python
def test_provider_internal_attribute():
    provider = OpenAIProvider()
    assert provider._internal_cache == {}  # Testing implementation detail
```

**Good** âœ…:
```python
def test_provider_interface():
    provider = OpenAIProvider()
    assert hasattr(provider, 'get_client')
    assert hasattr(provider, 'summarize')
    assert callable(provider.summarize)  # Test public interface
```

### 4. Mock External Dependencies

**Bad** âŒ:
```python
def test_summarization():
    provider = OpenAIProvider()
    result = provider.summarize(...)  # Makes real API call!
```

**Good** âœ…:
```python
def test_summarization(mock_openai_client):
    provider = OpenAIProvider()
    result = provider.summarize(client=mock_openai_client, ...)  # Uses mock
```

### 5. Test Error Handling

```python
def test_handles_api_error(mock_openai_client):
    # Configure mock to raise exception
    mock_openai_client.chat.completions.create.side_effect = Exception("API Error")
    
    provider = OpenAIProvider()
    result = provider.summarize(client=mock_openai_client, ...)
    
    # Should handle gracefully, not crash
    assert "error" in result.lower()
```

### 6. Use Descriptive Test Names

**Bad** âŒ:
```python
def test_1():
    ...

def test_provider():
    ...
```

**Good** âœ…:
```python
def test_ollama_uses_default_model_when_none_specified():
    ...

def test_provider_handles_missing_api_key_gracefully():
    ...
```

## ğŸ”§ Common Commands

```bash
# Run tests with output
pytest -v

# Run tests with print statements
pytest -s

# Run specific test file
pytest llm_core/tests/test_providers.py

# Run tests matching a pattern
pytest -k "test_openai"

# Run with coverage
pytest --cov=llm_core --cov-report=term-missing

# Generate HTML coverage report
pytest --cov=llm_core --cov-report=html
open htmlcov/index.html
```

## ğŸ“š Learning Resources

### Pytest Documentation
- [Official Pytest Docs](https://docs.pytest.org/)
- [Fixtures Guide](https://docs.pytest.org/en/stable/fixture.html)
- [Parametrization](https://docs.pytest.org/en/stable/parametrize.html)

### Testing Best Practices
- [Testing Best Practices (Real Python)](https://realpython.com/pytest-python-testing/)
- [Effective Python Testing](https://effectivepython.com/)
- [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development)

## âœ… Summary

**Key Takeaways**:
1. âœ… Write unit tests first (fast, reliable)
2. âœ… Use fixtures to DRY up test setup
3. âœ… Mock external dependencies (APIs, databases)
4. âœ… Test behavior, not implementation
5. âœ… Use integration tests sparingly (slow, expensive)
6. âœ… Organize tests into clear categories
7. âœ… Aim for meaningful coverage, not 100%

**Remember**: Good tests make refactoring safe and documentation clear!
